{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1939df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanmay Vijay Jadhav (20121040)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca3251-6660-4c11-ab9e-607923eafcb8",
   "metadata": {},
   "source": [
    "Assignment 7 - Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2452a52c-93a8-42e2-8a93-713c5a9e53e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample document created by Tanmay Jadhav for Assignment No. 7\n"
     ]
    }
   ],
   "source": [
    "# Importing our library and reading the doc file\n",
    "import docx\n",
    "doc = docx.Document('C:\\\\Users\\\\Acer\\\\Desktop\\\\DSBDA Self\\\\Assignment 7\\\\Sample Document.docx')\n",
    "\n",
    "# Printing the title\n",
    "print(doc.paragraphs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ab6639-a2f1-43a4-8977-9b87de430e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7b410-f8b8-4bd4-b4ad-16545bae3fda",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a40e87-a97e-48c1-96dc-3fbc5e89d340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'originated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'readers',\n",
       " 'who',\n",
       " 'prefer',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'skills',\n",
       " 'from',\n",
       " 'the',\n",
       " 'comforts',\n",
       " 'of',\n",
       " 'their',\n",
       " 'drawing',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "#word tokenize(s)\n",
    "nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136cbddc-4d4d-4e19-bb8e-2bcd5dabd59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'document',\n",
       " 'created',\n",
       " 'by',\n",
       " 'Tanmay',\n",
       " 'Jadhav',\n",
       " 'for',\n",
       " 'Assignment',\n",
       " 'No',\n",
       " '.',\n",
       " '7']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "doc=docx.Document('C:\\\\Users\\\\Acer\\\\Desktop\\\\DSBDA Self\\\\Assignment 7\\\\Sample Document.docx')\n",
    "d=doc.paragraphs[0].text\n",
    "nltk.word_tokenize(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83394c-22e1-4cb0-a43f-baa7dcfe54fb",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f2b17c6-e4dc-4834-8648-65c4b349a354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After split :  ['learn', 'php', 'from', 'GCOEARA', 'and', 'make', 'study', 'easy']\n",
      "After token :  [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('GCOEARA', 'NNP'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "#import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"learn php from GCOEARA and make study easy\".split()\n",
    "print(\"After split : \", text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After token : \", tokens_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a416d00-3e22-4f87-98a5-d2a69b15d6e1",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6905544-9c9f-49aa-be2c-8a607acfefd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7cd92ba-d573-49eb-a31c-0eff3c280bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a68df-d221-42c6-9e57-f95742f0df80",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8414ffed-268d-41a5-8f95-c51bc7002660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sentence = \"Programmers program with programming languages\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "\tprint(w,\" : \",  ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab57194-e342-45b9-b021-2ad3a1a06d99",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c85bf576-78ed-428e-bdf2-85542add4bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks :  rock\n",
      "corpora :  corpus\n",
      "better :  good\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks : \", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora : \", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better : \", lemmatizer.lemmatize(\"better\" ,pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eb7e0106-34b2-48ad-82d4-cbffd06fa932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job', 'learning', '21st', 'for', 'key', 'sexiest', 'data', 'science', 'Data', 'Science', 'of', 'machine', 'is', 'century', 'the'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "first_sentence = \"Data Science is the sexiest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")\n",
    "#join them to remove comon duplicate words\n",
    "total = set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d7580f3-4a27-4b9c-9906-b7c3602dc076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>learning</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>key</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>of</th>\n",
       "      <th>machine</th>\n",
       "      <th>is</th>\n",
       "      <th>century</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job  learning  21st  for  key  sexiest  data  science  Data  Science  of  \\\n",
       "0    1         0     1    0    0        1     0        0     1        1   1   \n",
       "1    0         1     0    1    1        0     1        1     0        0   0   \n",
       "\n",
       "   machine  is  century  the  \n",
       "0        0   1        1    2  \n",
       "1        1   1        0    1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "\twordDictB[word]+=1\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ea25ab26-62cd-4b53-ae67-3b88114483a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>learning</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>key</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>of</th>\n",
       "      <th>machine</th>\n",
       "      <th>is</th>\n",
       "      <th>century</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job  learning  21st    for    key  sexiest   data  science  Data  Science  \\\n",
       "0  0.1     0.000   0.1  0.000  0.000      0.1  0.000    0.000   0.1      0.1   \n",
       "1  0.0     0.125   0.0  0.125  0.125      0.0  0.125    0.125   0.0      0.0   \n",
       "\n",
       "    of  machine     is  century    the  \n",
       "0  0.1    0.000  0.100      0.1  0.200  \n",
       "1  0.0    0.125  0.125      0.0  0.125  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd86887-e987-4f96-bb99-61cf730d1171",
   "metadata": {},
   "source": [
    "#### we move onto the IDF part and implement the idf formula, letâ€™s finish with calculating the TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39bf47d8-7b54-40ca-afa4-61e81b8de57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job  learning     21st      for      key  sexiest     data  science  \\\n",
      "0  0.30103   0.00000  0.30103  0.00000  0.00000  0.30103  0.00000  0.00000   \n",
      "1  0.00000   0.30103  0.00000  0.30103  0.30103  0.00000  0.30103  0.30103   \n",
      "\n",
      "      Data  Science       of  machine       is  century      the  \n",
      "0  0.30103  0.30103  0.30103  0.00000  0.30103  0.30103  0.60206  \n",
      "1  0.00000  0.00000  0.00000  0.30103  0.30103  0.00000  0.30103  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    for doc in docList:\n",
    "        for word in doc:\n",
    "            if word not in idfDict:\n",
    "                idfDict[word] = 0\n",
    "            idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val)+ 1)\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    N = len(tfBow)\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs.get(word, 0)\n",
    "    return tfidf\n",
    "\n",
    "# Example usage\n",
    "docList = [wordDictA, wordDictB]\n",
    "idfs = computeIDF(docList)\n",
    "\n",
    "tfidfA = computeTFIDF(wordDictA, idfs)\n",
    "tfidfB = computeTFIDF(wordDictB, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06262f4-b9b2-4b47-9f00-db25a6f79ea9",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
